{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36078d9-c788-4323-b9af-88225e6c6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL, KandinskyV22PriorPipeline\n",
    "from accelerate.utils import set_seed\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, MultiScaleStructuralSimilarityIndexMeasure\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.dataset import MetricsDataset\n",
    "from src.clip_image_score import CLIPImageScore\n",
    "\n",
    "from src.ip_adapter import IPAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab924c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083755d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = 'dreamlike-art/dreamlike-anime-1.0'\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"/home/chaichuk/Annual_Project/IP-Adapter/models/image_encoder\"\n",
    "prior_model_path = \"kandinsky-community/kandinsky-2-2-prior\"\n",
    "ip_ckpt = \"/home/chaichuk/Annual_Project/weights/512_res_model/checkpoint-100/ip_adapter.bin\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ac0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    pil_face_image = [example[\"pil_face_image\"] for example in data]\n",
    "    pt_face_image = torch.stack([example[\"pt_face_image\"] for example in data])\n",
    "    anime_image = torch.stack([example[\"anime_image\"] for example in data])\n",
    "    text = [example[\"text\"] for example in data]\n",
    "\n",
    "    return {\n",
    "        \"pil_face_image\": pil_face_image,\n",
    "        \"pt_face_image\": pt_face_image,\n",
    "        \"anime_image\": anime_image,\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "dataset = MetricsDataset(num_samples=10000)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=50,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f5468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512ac488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4093dff1004a48847bd71f862bcc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ffb71a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaichuk/Annual_Project/Team73-Annual-Project/src/ip_adapter/ip_adapter.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "ip_model = IPAdapter(pipeline, image_encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146d2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d59f56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7379e4f06d074ea0b08a050be9043cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (87 > 77). Running this sequence through the model will result in indexing errors\n",
      "/home/chaichuk/annual-project-venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered caption longer than max_position_embeddings=77. Will truncate captions to this length.If longer captions are needed, initialize argument `model_name_or_path` with a model that supportslonger sequences\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n",
    "clip_image_score = CLIPImageScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    texts = batch[\"text\"]\n",
    "    pil_face_images = batch[\"pil_face_image\"]\n",
    "    pt_face_images = batch[\"pt_face_image\"]\n",
    "    anime_images = batch[\"anime_image\"]\n",
    "\n",
    "    fid.update(anime_images.to(device), real=True)\n",
    "\n",
    "    images = ip_model.generate(pil_image=pil_face_images, prompt='one person\\'s face, anime style', num_samples=1, num_inference_steps=50, height=512, width=512, scale=0.7, output_type='pt')\n",
    "    ssim.update(images.to(torch.float32), pt_face_images.to(device))\n",
    "    ms_ssim.update(images.to(torch.float32), pt_face_images.to(device))\n",
    "    fid.update(images.to(torch.float32), real=False)\n",
    "    clip_score.update(images, texts)\n",
    "    clip_image_score.update(pt_face_images.to(device), images.to(torch.float32))\n",
    "\n",
    "\n",
    "fid_value = fid.compute().to('cpu')\n",
    "ssim_value = ssim.compute().to('cpu')\n",
    "ms_ssim_value = ms_ssim.compute().to('cpu')\n",
    "clip_score_value = clip_score.compute().to('cpu')\n",
    "clip_image_score_value = clip_image_score.compute().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4131a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 145.1024169921875\n",
      "CLIP-T Score: 22.548322677612305\n",
      "CLIP-I Score: 98.7083969116211\n",
      "SSIM: 0.3494286835193634\n",
      "MS_SSIM: 0.2425501048564911\n"
     ]
    }
   ],
   "source": [
    "print('FID:', float(fid_value))\n",
    "print('CLIP-T Score:', float(clip_score_value))\n",
    "print('CLIP-I Score:', float(clip_image_score_value))\n",
    "\n",
    "print('SSIM:', float(ssim_value))\n",
    "print('MS_SSIM:', float(ms_ssim_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
